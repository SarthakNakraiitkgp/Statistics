{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59ac0f3-072f-4f26-a667-062afe78a735",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ab09c-c5e3-4d4a-8a5e-b4a9bc33d47c",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical test used to determine whether there are any significant differences between means of two or more groups. The basic assumptions of ANOVA are as follows:\n",
    "\n",
    "Normality: The data should be normally distributed within each group.\n",
    "\n",
    "Homogeneity of variance: The variance of the data in each group should be approximately equal.\n",
    "\n",
    "Independence: The observations in each group should be independent of each other.\n",
    "\n",
    "If any of these assumptions are violated, the results of the ANOVA may not be valid. Here are some examples of violations that could impact the validity of the results:\n",
    "\n",
    "Non-normality: If the data in any of the groups is not normally distributed, this can affect the validity of the ANOVA results. For example, if the data in one group is skewed, the mean of that group may not be a good representation of the data.\n",
    "\n",
    "Heterogeneity of variance: If the variance of the data in each group is significantly different, this can affect the validity of the ANOVA results. For example, if one group has a much larger variance than the other groups, this can make it more difficult to detect differences between means.\n",
    "\n",
    "Dependence: If the observations in any of the groups are not independent, this can affect the validity of the ANOVA results. For example, if the observations in one group are paired (e.g., before and after measurements), this violates the independence assumption and can lead to spurious results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6c7b0-ce0e-42b6-b473-aa91d83c755e",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7afcafa-d086-4c94-9e3b-df8f35e6f476",
   "metadata": {},
   "source": [
    "There are three types of ANOVA:\n",
    "\n",
    "1. One-way ANOVA: This type of ANOVA is used when there is only one independent variable, which has three or more levels (or groups). One-way ANOVA is used to determine if there are any significant differences between the means of the different levels/groups. For example, one-way ANOVA can be used to compare the average weight loss among three different diet groups.\n",
    "\n",
    "2. Two-way ANOVA: This type of ANOVA is used when there are two independent variables, both of which have two or more levels. Two-way ANOVA is used to determine if there are any significant main effects (i.e., the effect of each independent variable on the dependent variable) and/or interaction effects (i.e., the combined effect of the two independent variables on the dependent variable). For example, two-way ANOVA can be used to compare the average sales of two different products (product A and product B) in two different regions (North and South).\n",
    "\n",
    "3. MANOVA (Multivariate Analysis of Variance): This type of ANOVA is used when there are two or more dependent variables (i.e., outcome variables) and one or more independent variables. MANOVA is used to determine if there are any significant differences between the means of the dependent variables across the different levels of the independent variable. For example, MANOVA can be used to compare the average scores on multiple personality traits (e.g., extroversion, agreeableness, neuroticism) between different age groups (young, middle-aged, and old)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d50a2-9802-449c-bada-121f9d7a3f10",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818495e2-7082-44b0-800d-3620b4b725b1",
   "metadata": {},
   "source": [
    "Partitioning of variance in ANOVA (Analysis of Variance) refers to the process of decomposing the total variation in the data into different sources of variation. In other words, ANOVA breaks down the total variation in the dependent variable into the variation explained by the independent variable(s) and the variation that is not explained by the independent variable(s). This is important because it helps us understand how much of the variation in the dependent variable can be attributed to the independent variable(s).\n",
    "\n",
    "The partitioning of variance in ANOVA is typically represented by the following equation:\n",
    "\n",
    "Total variation = Variation explained by independent variable(s) + Variation not explained by independent variable(s)\n",
    "\n",
    "The variation explained by the independent variable(s) is referred to as the \"between-group\" variation, while the variation not explained by the independent variable(s) is referred to as the \"within-group\" variation.\n",
    "\n",
    "Understanding the partitioning of variance is important in ANOVA because it helps us determine if there is a significant difference between the means of the different groups being compared. If the variation explained by the independent variable(s) (i.e., the between-group variation) is much larger than the variation not explained by the independent variable(s) (i.e., the within-group variation), then it suggests that there is a significant difference between the means of the groups being compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316be1b-ecae-4ad1-ad6a-8617dfe7dd93",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6859a232-bc0c-4622-88d4-0dbfe0a2e84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST = 223.33333333333337\n",
      "SSE = 223.33333333333337\n",
      "SSR = 0.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# create three sample groups\n",
    "group1 = [10, 12, 14, 16, 18]\n",
    "group2 = [8, 11, 14, 17, 20]\n",
    "group3 = [9, 12, 15, 18, 21]\n",
    "\n",
    "# concatenate the groups\n",
    "data = group1 + group2 + group3\n",
    "\n",
    "# calculate the mean of the data\n",
    "mean = sum(data) / len(data)\n",
    "\n",
    "# calculate the total sum of squares (SST)\n",
    "SST = sum([(x - mean)**2 for x in data])\n",
    "\n",
    "# calculate the sum of squares between (SSB)\n",
    "SSB = len(group1) * (sum([(x - mean)**2 for x in group1]) / len(group1))\n",
    "SSB += len(group2) * (sum([(x - mean)**2 for x in group2]) / len(group2))\n",
    "SSB += len(group3) * (sum([(x - mean)**2 for x in group3]) / len(group3))\n",
    "\n",
    "# calculate the explained sum of squares (SSE)\n",
    "SSE = SSB\n",
    "\n",
    "# calculate the residual sum of squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"SST =\", SST)\n",
    "print(\"SSE =\", SSE)\n",
    "print(\"SSR =\", SSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe09b5-6148-4e57-9cf2-4775ac5c608a",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541e572a-3260-4b18-9105-d3554c9dd874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effects: C(data1)[T.B]    5.0\n",
      "C(data2)[T.Y]    2.0\n",
      "dtype: float64\n",
      "Interaction effect: 1.0\n"
     ]
    }
   ],
   "source": [
    "# dataset with two categorical variables and one continuous variable\n",
    "df = pd.DataFrame({'data1': ['A', 'A', 'B', 'B'], \n",
    "                   'data2': ['X', 'Y', 'X', 'Y'],\n",
    "                   'value': [10.0, 12.0, 15.0, 18.0]})\n",
    "\n",
    "# fit a two-way ANOVA model with interaction effect\n",
    "model = ols('value ~ C(data1) + C(data2) + C(data1):C(data2)', data=df).fit()\n",
    "\n",
    "# extract the main effects and interaction effect\n",
    "effects = model.params[1:3]\n",
    "int_effect = model.params[3]\n",
    "\n",
    "print(\"Main effects:\", effects)\n",
    "print(\"Interaction effect:\", int_effect.round())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856fc24-9c35-4fc7-b053-f82e21e72f13",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5056d-8e6c-472f-8c89-9df99259fad0",
   "metadata": {},
   "source": [
    "If we conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02, we can conclude that there is at least one significant difference between the means of the groups being compared. The F-statistic tells us the ratio of the variance between the groups to the variance within the groups. A larger F-statistic indicates a greater difference between the group means compared to the variation within the groups. The p-value tells us the probability of obtaining a result as extreme as the observed result if there were no true differences between the groups.\n",
    "\n",
    "With a p-value of 0.02, we can interpret this result as follows: if there were no true differences between the groups, we would only expect to obtain a result as extreme as an F-statistic of 5.23 or higher by chance 2% of the time. Therefore, we reject the null hypothesis (that there are no true differences between the groups) and conclude that there is sufficient evidence to suggest that at least one of the group means is different from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8900905-1baa-48a6-a1ac-174222256604",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f548d-30c8-44e1-9ae1-245707d79146",
   "metadata": {},
   "source": [
    "In a repeated measures ANOVA, missing data can be handled in several ways:\n",
    "\n",
    "Listwise deletion: This method involves excluding any cases with missing data from the analysis. This can be done using the dropna() function in pandas. While this approach is simple, it may result in a loss of statistical power if a large amount of data is missing.\n",
    "\n",
    "Mean imputation: This method involves replacing missing values with the mean of the non-missing values. This can be done using the fillna() function in pandas. While this approach is simple and easy to implement, it may underestimate the variability of the data and result in biased estimates.\n",
    "\n",
    "Last observation carried forward (LOCF): This method involves imputing missing values with the last observed value. This can be done using the fillna(method='ffill') function in pandas. While this approach is useful for data with a temporal order, it may not be appropriate for all situations and may result in biased estimates.\n",
    "\n",
    "Multiple imputation: This method involves imputing missing values multiple times using a statistical model, and then combining the results to obtain estimates and standard errors. This can be done using the fancyimpute library in Python. While this approach is more sophisticated and can produce more accurate estimates than mean imputation or LOCF, it is computationally intensive and requires careful consideration of the underlying assumptions.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data in a repeated measures ANOVA include bias in the estimated means, standard errors, and effect sizes, as well as a loss of statistical power. It's important to carefully consider the underlying assumptions and potential limitations of each method and choose the approach that is most appropriate for the specific dataset and research question. Additionally, it may be beneficial to conduct sensitivity analyses to assess the robustness of the results to different methods of handling missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b5c17-2972-4302-a587-4b055f791b2b",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e3e84-6259-43ab-8206-c009752b33ed",
   "metadata": {},
   "source": [
    "Post-hoc tests are used after ANOVA to make pairwise comparisons between groups when the overall ANOVA result is statistically significant. The purpose of post-hoc tests is to determine which specific groups differ from each other and to control for the familywise error rate, which is the probability of making at least one Type I error (false positive) across all the pairwise comparisons. Here are some common post-hoc tests used after ANOVA, along with an example of a situation where each one might be necessary:\n",
    "\n",
    "Tukey's HSD (honestly significant difference) test: This test is a conservative post-hoc test that is commonly used when the sample sizes are equal across groups. It controls for the familywise error rate by adjusting the significance level for each pairwise comparison. For example, if we have four groups (A, B, C, D), and the overall ANOVA result is significant, we might use Tukey's HSD test to determine which specific groups differ from each other. If the test shows that group A is significantly different from group B and group C, but not group D, we can conclude that group A is significantly different from groups B and C, but not group D.\n",
    "\n",
    "Bonferroni correction: This test is a simple and commonly used method to adjust the significance level for each pairwise comparison. It divides the significance level (usually 0.05) by the number of comparisons being made. For example, if we have four groups (A, B, C, D), and the overall ANOVA result is significant, we might use the Bonferroni correction to determine which specific groups differ from each other. If the test shows that group A is significantly different from group B, group C, and group D, we can conclude that group A is significantly different from all the other groups.\n",
    "\n",
    "Dunnett's test: This test is used when we have one control group and several treatment groups. It compares each treatment group to the control group, while controlling for the overall familywise error rate. For example, if we have one control group and three treatment groups (A, B, C), and the overall ANOVA result is significant, we might use Dunnett's test to determine which specific treatment groups differ from the control group. If the test shows that group A is significantly different from the control group, but groups B and C are not significantly different from the control group, we can conclude that group A is significantly different from the control group, but groups B and C are not.\n",
    "\n",
    "Scheffe's test: This test is a conservative post-hoc test that is used when the sample sizes are unequal across groups. It controls for the familywise error rate by adjusting the significance level for each pairwise comparison. For example, if we have four groups (A, B, C, D), and the overall ANOVA result is significant, we might use Scheffe's test to determine which specific groups differ from each other. If the test shows that group A is significantly different from group B and group C, but not group D, we can conclude that group A is significantly different from groups B and C, but not group D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d270a0a-5c22-43f8-888c-6d442ff6ea5d",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e151e6-07ca-41d0-afcf-73364541a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 176.89689491604346\n",
      "p-value: 1.63227843737611e-28\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data\n",
    "diet_A = [4.2, 5.1, 3.7, 6.2, 4.9, 2.8, 5.4, 4.7, 3.9, 4.1,\n",
    "          3.3, 5.5, 4.8, 5.3, 3.9, 6.1, 4.3, 3.5, 5.2, 5.0,\n",
    "          5.6, 3.8, 5.8, 4.0, 5.7]\n",
    "diet_B = [2.9, 1.9, 2.5, 2.3, 3.2, 2.7, 1.8, 3.1, 2.6, 3.0,\n",
    "          2.4, 2.0, 2.8, 2.2, 2.1, 2.6, 2.7, 1.6, 2.4, 2.9,\n",
    "          3.0, 1.8, 2.1, 2.3, 2.6]\n",
    "diet_C = [1.3, 0.9, 1.1, 1.6, 1.4, 1.8, 1.2, 1.5, 2.0, 1.4,\n",
    "          1.7, 1.3, 1.9, 1.0, 1.6, 1.2, 1.8, 1.4, 1.1, 1.7,\n",
    "          1.5, 1.6, 1.9, 1.2, 1.4]\n",
    "\n",
    "# One-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b322c-3c48-4e45-95fb-5268435d7b31",
   "metadata": {},
   "source": [
    "Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d18d6f3-e851-4ce7-9eb2-707d7dfd4b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     sum_sq    df         F    PR(>F)\n",
      "C(Software_name)                  46.866667   2.0  0.709760  0.494687\n",
      "C(Experience)                      2.500000   1.0  0.075721  0.783858\n",
      "C(Software_name):C(Experience)    25.800000   2.0  0.390721  0.677792\n",
      "Residual                        2773.333333  84.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "# Create a sample dataset\n",
    "df = pd.DataFrame({'Software_name': ['A', 'B', 'C'] * 30,\n",
    "                     'Experience': ['Novice'] * 45 + ['Experienced'] * 45,\n",
    "                     'Time': np.random.randint(10,30,90)})\n",
    "\n",
    "# Fit a two-way ANOVA model\n",
    "model = ols('Time ~ C(Software_name) + C(Experience) + C(Software_name):C(Experience)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n",
    "\n",
    "# data['Time'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446743e5-b1b3-4fdf-b9b6-cef88f23f46e",
   "metadata": {},
   "source": [
    "Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da050d91-abd5-42cd-9793-565f2c39874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: 0.4209369001827434\n",
      "p-value: 0.6742577032092383\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "=====================================================\n",
      "group1 group2 meandiff p-adj   lower    upper  reject\n",
      "-----------------------------------------------------\n",
      "    70     71  -0.3333    1.0 -12.1112 11.4446  False\n",
      "    70     72  -1.1905    1.0 -11.8319   9.451  False\n",
      "    70     73  -3.6667 0.9996 -14.7957  7.4624  False\n",
      "    70     74  -0.3333    1.0 -13.0224 12.3558  False\n",
      "    70     75  -1.8333    1.0 -12.0938  8.4272  False\n",
      "    70     76  -0.6667    1.0  -14.744 13.4106  False\n",
      "    70     77  -0.4167    1.0  -9.7279  8.8946  False\n",
      "    70     78     -2.0    1.0 -16.0773 12.0773  False\n",
      "    70     79   2.4667    1.0  -9.3112 14.2446  False\n",
      "    70     80  -6.0833 0.9647 -18.7724  6.6058  False\n",
      "    70     81  -4.8333 0.9999 -21.3404 11.6738  False\n",
      "    70     82  -3.6667    1.0  -17.744 10.4106  False\n",
      "    70     83  -2.8333    1.0 -19.3404 13.6738  False\n",
      "    70     84  -5.5833 0.9851 -18.2724  7.1058  False\n",
      "    70     85      4.0 0.9999 -10.0773 18.0773  False\n",
      "    70     86  -0.8333    1.0 -11.9624 10.2957  False\n",
      "    70     87  -7.3333 0.9835 -23.8404  9.1738  False\n",
      "    70     88     -3.0    1.0 -14.1291  8.1291  False\n",
      "    70     89  -1.6667    1.0 -12.7957  9.4624  False\n",
      "    71     72  -0.8571    1.0 -13.2214 11.5071  False\n",
      "    71     73  -3.3333    1.0 -16.1197   9.453  False\n",
      "    71     74      0.0    1.0  -14.165  14.165  False\n",
      "    71     75     -1.5    1.0 -13.5379 10.5379  False\n",
      "    71     76  -0.3333    1.0 -15.7542 15.0876  False\n",
      "    71     77  -0.0833    1.0 -11.3231 11.1565  False\n",
      "    71     78  -1.6667    1.0 -17.0876 13.7542  False\n",
      "    71     79      2.8    1.0 -10.5549 16.1549  False\n",
      "    71     80    -5.75 0.9939  -19.915   8.415  False\n",
      "    71     81     -4.5    1.0 -22.1669 13.1669  False\n",
      "    71     82  -3.3333    1.0 -18.7542 12.0876  False\n",
      "    71     83     -2.5    1.0 -20.1669 15.1669  False\n",
      "    71     84    -5.25  0.998  -19.415   8.915  False\n",
      "    71     85   4.3333    1.0 -11.0876 19.7542  False\n",
      "    71     86     -0.5    1.0 -13.2863 12.2863  False\n",
      "    71     87     -7.0 0.9954 -24.6669 10.6669  False\n",
      "    71     88  -2.6667    1.0  -15.453 10.1197  False\n",
      "    71     89  -1.3333    1.0 -14.1197  11.453  False\n",
      "    72     73  -2.4762    1.0  -14.224  9.2716  False\n",
      "    72     74   0.8571    1.0  -12.378 14.0923  False\n",
      "    72     75  -0.6429    1.0 -11.5714 10.2857  False\n",
      "    72     76   0.5238    1.0 -14.0476 15.0952  False\n",
      "    72     77   0.7738    1.0  -9.2688 10.8164  False\n",
      "    72     78  -0.8095    1.0 -15.3809 13.7619  False\n",
      "    72     79   3.6571 0.9999  -8.7071 16.0214  False\n",
      "    72     80  -4.8929  0.998  -18.128  8.3423  False\n",
      "    72     81  -3.6429    1.0 -20.5733 13.2876  False\n",
      "    72     82  -2.4762    1.0 -17.0476 12.0952  False\n",
      "    72     83  -1.6429    1.0 -18.5733 15.2876  False\n",
      "    72     84  -4.3929 0.9995  -17.628  8.8423  False\n",
      "    72     85   5.1905 0.9988  -9.3809 19.7619  False\n",
      "    72     86   0.3571    1.0 -11.3907  12.105  False\n",
      "    72     87  -6.1429 0.9984 -23.0733 10.7876  False\n",
      "    72     88  -1.8095    1.0 -13.5573  9.9383  False\n",
      "    72     89  -0.4762    1.0  -12.224 11.2716  False\n",
      "    73     74   3.3333    1.0 -10.2969 16.9636  False\n",
      "    73     75   1.8333    1.0  -9.5706 13.2372  False\n",
      "    73     76      3.0    1.0 -11.9312 17.9312  False\n",
      "    73     77     3.25 0.9998   -7.308  13.808  False\n",
      "    73     78   1.6667    1.0 -13.2646 16.5979  False\n",
      "    73     79   6.1333 0.9645   -6.653 18.9197  False\n",
      "    73     80  -2.4167    1.0 -16.0469 11.2136  False\n",
      "    73     81  -1.1667    1.0 -18.4078 16.0744  False\n",
      "    73     82      0.0    1.0 -14.9312 14.9312  False\n",
      "    73     83   0.8333    1.0 -16.4078 18.0744  False\n",
      "    73     84  -1.9167    1.0 -15.5469 11.7136  False\n",
      "    73     85   7.6667 0.9345  -7.2646 22.5979  False\n",
      "    73     86   2.8333    1.0   -9.358 15.0246  False\n",
      "    73     87  -3.6667    1.0 -20.9078 13.5744  False\n",
      "    73     88   0.6667    1.0 -11.5246  12.858  False\n",
      "    73     89      2.0    1.0 -10.1913 14.1913  False\n",
      "    74     75     -1.5    1.0 -14.4308 11.4308  False\n",
      "    74     76  -0.3333    1.0 -16.4609 15.7942  False\n",
      "    74     77  -0.0833    1.0 -12.2746  12.108  False\n",
      "    74     78  -1.6667    1.0 -17.7942 14.4609  False\n",
      "    74     79      2.8    1.0  -11.365  16.965  False\n",
      "    74     80    -5.75 0.9967 -20.6812  9.1812  False\n",
      "    74     81     -4.5    1.0 -22.7869 13.7869  False\n",
      "    74     82  -3.3333    1.0 -19.4609 12.7942  False\n",
      "    74     83     -2.5    1.0 -20.7869 15.7869  False\n",
      "    74     84    -5.25  0.999 -20.1812  9.6812  False\n",
      "    74     85   4.3333    1.0 -11.7942 20.4609  False\n",
      "    74     86     -0.5    1.0 -14.1303 13.1303  False\n",
      "    74     87     -7.0  0.997 -25.2869 11.2869  False\n",
      "    74     88  -2.6667    1.0 -16.2969 10.9636  False\n",
      "    74     89  -1.3333    1.0 -14.9636 12.2969  False\n",
      "    75     76   1.1667    1.0 -13.1289 15.4622  False\n",
      "    75     77   1.4167    1.0  -8.2214 11.0547  False\n",
      "    75     78  -0.1667    1.0 -14.4622 14.1289  False\n",
      "    75     79      4.3 0.9987  -7.7379 16.3379  False\n",
      "    75     80    -4.25 0.9996 -17.1808  8.6808  False\n",
      "    75     81     -3.0    1.0 -19.6936 13.6936  False\n",
      "    75     82  -1.8333    1.0 -16.1289 12.4622  False\n",
      "    75     83     -1.0    1.0 -17.6936 15.6936  False\n",
      "    75     84    -3.75 0.9999 -16.6808  9.1808  False\n",
      "    75     85   5.8333 0.9935  -8.4622 20.1289  False\n",
      "    75     86      1.0    1.0 -10.4039 12.4039  False\n",
      "    75     87     -5.5 0.9996 -22.1936 11.1936  False\n",
      "    75     88  -1.1667    1.0 -12.5706 10.2372  False\n",
      "    75     89   0.1667    1.0 -11.2372 11.5706  False\n",
      "    76     77     0.25    1.0 -13.3803 13.8803  False\n",
      "    76     78  -1.3333    1.0 -18.5744 15.9078  False\n",
      "    76     79   3.1333    1.0 -12.2876 18.5542  False\n",
      "    76     80  -5.4167 0.9994 -21.5442 10.7109  False\n",
      "    76     81  -4.1667    1.0 -23.4428 15.1095  False\n",
      "    76     82     -3.0    1.0 -20.2411 14.2411  False\n",
      "    76     83  -2.1667    1.0 -21.4428 17.1095  False\n",
      "    76     84  -4.9167 0.9999 -21.0442 11.2109  False\n",
      "    76     85   4.6667    1.0 -12.5744 21.9078  False\n",
      "    76     86  -0.1667    1.0 -15.0979 14.7646  False\n",
      "    76     87  -6.6667 0.9992 -25.9428 12.6095  False\n",
      "    76     88  -2.3333    1.0 -17.2646 12.5979  False\n",
      "    76     89     -1.0    1.0 -15.9312 13.9312  False\n",
      "    77     78  -1.5833    1.0 -15.2136 12.0469  False\n",
      "    77     79   2.8833    1.0  -8.3565 14.1231  False\n",
      "    77     80  -5.6667 0.9738  -17.858  6.5246  False\n",
      "    77     81  -4.4167    1.0 -20.5442 11.7109  False\n",
      "    77     82    -3.25    1.0 -16.8803 10.3803  False\n",
      "    77     83  -2.4167    1.0 -18.5442 13.7109  False\n",
      "    77     84  -5.1667   0.99  -17.358  7.0246  False\n",
      "    77     85   4.4167 0.9997  -9.2136 18.0469  False\n",
      "    77     86  -0.4167    1.0 -10.9746 10.1413  False\n",
      "    77     87  -6.9167 0.9887 -23.0442  9.2109  False\n",
      "    77     88  -2.5833    1.0 -13.1413  7.9746  False\n",
      "    77     89    -1.25    1.0  -11.808   9.308  False\n",
      "    78     79   4.4667 0.9999 -10.9542 19.8876  False\n",
      "    78     80  -4.0833    1.0 -20.2109 12.0442  False\n",
      "    78     81  -2.8333    1.0 -22.1095 16.4428  False\n",
      "    78     82  -1.6667    1.0 -18.9078 15.5744  False\n",
      "    78     83  -0.8333    1.0 -20.1095 18.4428  False\n",
      "    78     84  -3.5833    1.0 -19.7109 12.5442  False\n",
      "    78     85      6.0 0.9991 -11.2411 23.2411  False\n",
      "    78     86   1.1667    1.0 -13.7646 16.0979  False\n",
      "    78     87  -5.3333    1.0 -24.6095 13.9428  False\n",
      "    78     88     -1.0    1.0 -15.9312 13.9312  False\n",
      "    78     89   0.3333    1.0 -14.5979 15.2646  False\n",
      "    79     80    -8.55 0.7842  -22.715   5.615  False\n",
      "    79     81     -7.3 0.9925 -24.9669 10.3669  False\n",
      "    79     82  -6.1333 0.9952 -21.5542  9.2876  False\n",
      "    79     83     -5.3 0.9999 -22.9669 12.3669  False\n",
      "    79     84    -8.05 0.8553  -22.215   6.115  False\n",
      "    79     85   1.5333    1.0 -13.8876 16.9542  False\n",
      "    79     86     -3.3    1.0 -16.0863  9.4863  False\n",
      "    79     87     -9.8 0.8786 -27.4669  7.8669  False\n",
      "    79     88  -5.4667  0.989  -18.253  7.3197  False\n",
      "    79     89  -4.1333 0.9997 -16.9197   8.653  False\n",
      "    80     81     1.25    1.0 -17.0369 19.5369  False\n",
      "    80     82   2.4167    1.0 -13.7109 18.5442  False\n",
      "    80     83     3.25    1.0 -15.0369 21.5369  False\n",
      "    80     84      0.5    1.0 -14.4312 15.4312  False\n",
      "    80     85  10.0833 0.7341  -6.0442 26.2109  False\n",
      "    80     86     5.25 0.9967  -8.3803 18.8803  False\n",
      "    80     87    -1.25    1.0 -19.5369 17.0369  False\n",
      "    80     88   3.0833    1.0 -10.5469 16.7136  False\n",
      "    80     89   4.4167 0.9997  -9.2136 18.0469  False\n",
      "    81     82   1.1667    1.0 -18.1095 20.4428  False\n",
      "    81     83      2.0    1.0 -19.1159 23.1159  False\n",
      "    81     84    -0.75    1.0 -19.0369 17.5369  False\n",
      "    81     85   8.8333 0.9773 -10.4428 28.1095  False\n",
      "    81     86      4.0    1.0 -13.2411 21.2411  False\n",
      "    81     87     -2.5    1.0 -23.6159 18.6159  False\n",
      "    81     88   1.8333    1.0 -15.4078 19.0744  False\n",
      "    81     89   3.1667    1.0 -14.0744 20.4078  False\n",
      "    82     83   0.8333    1.0 -18.4428 20.1095  False\n",
      "    82     84  -1.9167    1.0 -18.0442 14.2109  False\n",
      "    82     85   7.6667 0.9833  -9.5744 24.9078  False\n",
      "    82     86   2.8333    1.0 -12.0979 17.7646  False\n",
      "    82     87  -3.6667    1.0 -22.9428 15.6095  False\n",
      "    82     88   0.6667    1.0 -14.2646 15.5979  False\n",
      "    82     89      2.0    1.0 -12.9312 16.9312  False\n",
      "    83     84    -2.75    1.0 -21.0369 15.5369  False\n",
      "    83     85   6.8333 0.9988 -12.4428 26.1095  False\n",
      "    83     86      2.0    1.0 -15.2411 19.2411  False\n",
      "    83     87     -4.5    1.0 -25.6159 16.6159  False\n",
      "    83     88  -0.1667    1.0 -17.4078 17.0744  False\n",
      "    83     89   1.1667    1.0 -16.0744 18.4078  False\n",
      "    84     85   9.5833 0.8045  -6.5442 25.7109  False\n",
      "    84     86     4.75 0.9991  -8.8803 18.3803  False\n",
      "    84     87    -1.75    1.0 -20.0369 16.5369  False\n",
      "    84     88   2.5833    1.0 -11.0469 16.2136  False\n",
      "    84     89   3.9167 0.9999  -9.7136 17.5469  False\n",
      "    85     86  -4.8333 0.9997 -19.7646 10.0979  False\n",
      "    85     87 -11.3333 0.8176 -30.6095  7.9428  False\n",
      "    85     88     -7.0 0.9715 -21.9312  7.9312  False\n",
      "    85     89  -5.6667 0.9973 -20.5979  9.2646  False\n",
      "    86     87     -6.5 0.9975 -23.7411 10.7411  False\n",
      "    86     88  -2.1667    1.0  -14.358 10.0246  False\n",
      "    86     89  -0.8333    1.0 -13.0246  11.358  False\n",
      "    87     88   4.3333    1.0 -12.9078 21.5744  False\n",
      "    87     89   5.6667 0.9996 -11.5744 22.9078  False\n",
      "    88     89   1.3333    1.0  -10.858 13.5246  False\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the test score data for each group\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "control_group = np.random.randint(70, 90, 100)        #traditional teaching method\n",
    "\n",
    "experimental_group = np.random.randint(70, 90, 100)   #new teaching method\n",
    "\n",
    "# two-sample t-test\n",
    "t_stat, p_val = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Print results\n",
    "print('t-statistic:', t_stat)\n",
    "print('p-value:', p_val)\n",
    "\n",
    "# Conduct post-hoc test (Tukey's HSD)\n",
    "f_stat, p_val = f_oneway(control_group, experimental_group)\n",
    "tukey_results = pairwise_tukeyhsd(control_group, experimental_group)\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8d5d8-5e16-4ed3-be9b-cf0d384b5fac",
   "metadata": {},
   "source": [
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5f724f-f25d-4ea9-833d-16d3513c51b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sum_sq    df         F    PR(>F)\n",
      "C(Stores)    53.088889   2.0  1.505094  0.230538\n",
      "C(Days)     637.155556  29.0  1.245769  0.234926\n",
      "Residual   1022.911111  58.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Create a sample dataset\n",
    "data = pd.DataFrame({'Days': list(range(1, 31)) * 3,\n",
    "                     'Stores': ['A'] * 30 + ['B'] * 30 + ['C'] * 30,\n",
    "                     'Sales': np.random.randint(10,25,90)})\n",
    "\n",
    "# Fit a repeated measures ANOVA model\n",
    "m = ols('Sales ~ C(Stores) + C(Days)', data=data).fit()\n",
    "a_table = sm.stats.anova_lm(m, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(a_table)\n",
    "# data['Sales'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07913e-36c8-4889-ae6d-57bceff3acee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
